{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "toxic_text_clf.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjGt81SJKHjc",
        "colab_type": "code",
        "outputId": "d5967389-53c9-41bd-9452-acb152d27b5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.layers import Dense, Input, LSTM, Bidirectional, Conv1D\n",
        "from keras.layers import Dropout, Embedding\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.layers import Dense, Input, LSTM, Bidirectional, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
        "from keras.layers import Dropout, Embedding, concatenate\n",
        "\n",
        "\n",
        "from keras.models import Model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now\n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zGN01cdfI1m",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNqSR4WQe9k2",
        "colab_type": "code",
        "outputId": "19073b83-debb-4921-f777-eb2b74eb325b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XV3Z0X4fWVI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !ls My Drive/Colab Notebooks/\n",
        "# gdrive/My Drive/Colab Notebooks/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyxtXu9Q4S_d",
        "colab_type": "code",
        "outputId": "da3f67cd-45f7-42e7-e746-a55a85e386cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\" DEPRECIATED --> Using training and GLOVE data straight from Google Drive \"\"\"\n",
        "\n",
        "\n",
        "# Import data from local drive\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' DEPRECIATED --> Using training and GLOVE data straight from Google Drive '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GN_0exaq427u",
        "colab_type": "code",
        "outputId": "eb7e4fb6-4a4c-4b67-a27a-61e117f3eb03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\" DEPRECIATED --> Using training and GLOVE data straight from Google Drive \"\"\"\n",
        "## load local data into CSV\n",
        "# import io\n",
        "# df = pd.read_csv(io.BytesIO(uploaded['train.csv'])).fillna(\" \")\n",
        "\n",
        "## Get Glove embeddings\n",
        "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "# !unzip glove*.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' DEPRECIATED --> Using training and GLOVE data straight from Google Drive '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgGV1MMFeL__",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get data from Google Drive\n",
        "df = pd.read_csv('gdrive/My Drive/Colab Notebooks/train.csv').fillna(\" \")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJjESNBkobM6",
        "colab_type": "code",
        "outputId": "b2714685-1e4b-413d-f99e-ce3e77880ad8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Replace NA values\n",
        "df['comment_text'].fillna(' ')\n",
        "\n",
        "# Separate input features and target\n",
        "y = df.loc[:, \"toxic\"]\n",
        "X = df.loc[:,'comment_text']\n",
        "\n",
        "\n",
        "# split into train and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)\n",
        "\n",
        "\n",
        "# concatenate our training data back together\n",
        "X = pd.concat([X_train, y_train], axis=1)\n",
        "\n",
        "# separate minority and majority classes\n",
        "non_toxic = X[X[\"toxic\"]==0]\n",
        "toxic = X[X[\"toxic\"]==1]\n",
        "\n",
        "# # upsample minority\n",
        "# toxic_upsampled = resample(toxic,\n",
        "#                           replace=True, # sample with replacement\n",
        "#                           n_samples=len(non_toxic), # match number in majority class\n",
        "#                           random_state=27) # reproducible results\n",
        "\n",
        "# # combine majority and upsampled minority\n",
        "# X_upsampled = pd.concat([non_toxic, toxic_upsampled])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# downsample majority\n",
        "non_toxic_downsampled = resample(non_toxic,\n",
        "                                replace = False, # sample without replacement\n",
        "                                n_samples = len(toxic), # match minority n\n",
        "                                random_state = 27) # reproducible results\n",
        "\n",
        "# combine minority and downsampled majority\n",
        "X_downsampled = pd.concat([non_toxic_downsampled, toxic])\n",
        "\n",
        "# Resampling for random order\n",
        "X_downsampled = X_downsampled.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "\n",
        "# AGAIN: Separate input features and target\n",
        "y = X_downsampled.loc[:, \"toxic\"]\n",
        "X = X_downsampled.loc[:,'comment_text']\n",
        "\n",
        "\n",
        "'''\n",
        "    QUESTION: Followed: https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18\n",
        "    for upsampling where it states to split before upsampling to avoid overfitting the data. After they split the data, they\n",
        "    resample the minority class, and then concatenate the results to a single Dataframe. In their model examples,\n",
        "    it seems as though they are training their models on the entirity of the dataset and resuing data that their model has\n",
        "    already seen for the test results, which should result in overfitting, right?\n",
        "    \n",
        "    So once I have the balanced upsampled dataset, I've resplit the dataset in training and testing\n",
        "\n",
        "'''\n",
        "\n",
        "# split into train and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "num_non_toxic = X_downsampled.loc[X_downsampled[\"toxic\"] == 0]\n",
        "num_toxic = X_downsampled.loc[X_downsampled[\"toxic\"] == 1]\n",
        "\n",
        "\n",
        "print(\"Class Distribution across 2 classes:\")\n",
        "print(len(num_non_toxic),len(num_toxic))\n",
        "\n",
        "\n",
        "max_features=100000\n",
        "maxlen=150\n",
        "embed_size=300\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class Distribution across 2 classes:\n",
            "11393 11393\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlSer0rRobQE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "    Tokenize split dataset (X_train, X_test)\n",
        "    USE FOR TRAINING MODELS INDIVIDUALLY\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Vectorize text + Prepare GloVe Embedding\n",
        "tokenizer = text.Tokenizer(num_words=max_features, lower=True)\n",
        "tokenizer.fit_on_texts(list(X_train))\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yKwXpdXobVe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "  LOADING AND INDEXING GLOVE WORD EMBEDDINGS (300-D)\n",
        "\"\"\"\n",
        "\n",
        "embeddings_index = {}\n",
        "with open('gdrive/My Drive/Colab Notebooks/glove.6B.300d.txt', encoding='utf8') as f:\n",
        "    for line in f:\n",
        "        values = line.rstrip().rsplit(' ')\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "num_words = min(max_features, len(word_index) + 1)\n",
        "embedding_matrix = np.zeros((num_words, embed_size))\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_features:\n",
        "        continue\n",
        "    \n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Em3u277Z6Mo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "    Tokenize the entire dataset\n",
        "    USE FOR CROSS VALIDATION \n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Vectorize text + Prepare GloVe Embedding\n",
        "tokenizer = text.Tokenizer(num_words=max_features, lower=True)\n",
        "tokenizer.fit_on_texts(list(X))\n",
        "\n",
        "import pickle\n",
        "\n",
        "\n",
        "with open('gdrive/My Drive/Colab Notebooks/text_tokenizer.pickle', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "\n",
        "X = tokenizer.texts_to_sequences(X)\n",
        "X = sequence.pad_sequences(X, maxlen=maxlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cj5kBjrhoba4",
        "colab_type": "code",
        "outputId": "a1a1cc2f-07c7-4411-a7bf-27ab024bb5a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\" LSTM MODEL WITH ATTENTION LAYER\"\"\"\n",
        "\n",
        "\n",
        "from keras.layers import Input, Flatten, Activation, multiply, TimeDistributed, RepeatVector, Permute, Lambda\n",
        "from keras import backend as K\n",
        "\n",
        "units = 32\n",
        "max_length = 150\n",
        "vocab_size = embedding_matrix.shape[0]\n",
        "embedding_size = embedding_matrix.shape[1]\n",
        "\n",
        "\n",
        "_input = Input(shape=[max_length], dtype='int32')\n",
        "\n",
        "# get the embedding layer\n",
        "embedded = Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=embedding_size,\n",
        "        input_length=max_length,\n",
        "        trainable=False,\n",
        "        weights=[embedding_matrix]\n",
        "    )(_input)\n",
        "\n",
        "# Bidirectional(LSTM(128, return_sequences=True))\n",
        "# activations = LSTM(units, return_sequences=True)(embedded)\n",
        "# Bidirectional(LSTM(64, return_sequences=True))\n",
        "activations = Bidirectional(LSTM(32, return_sequences=True))(embedded)\n",
        "\n",
        "\n",
        "# compute importance for each step\n",
        "attention = TimeDistributed(Dense(1, activation='tanh'))(activations) \n",
        "attention = Flatten()(attention)\n",
        "attention = Activation('softmax')(attention)\n",
        "# adjusting vector shapes for multiplication\n",
        "attention = RepeatVector(64)(attention)\n",
        "attention = Permute([2, 1])(attention)\n",
        "\n",
        "# apply the attention\n",
        "\n",
        "sent_representation = multiply([activations, attention])\n",
        "\n",
        "sent_representation = Lambda(lambda xin: K.sum(xin, axis=1))(sent_representation)\n",
        "\n",
        "probabilities = Dense(1, activation='sigmoid')(sent_representation)\n",
        "\n",
        "model = Model(input=_input, output=probabilities)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuOnWhUscDBO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# len(y)//10\n",
        "non_toxic_samples, non_toxic_labels = pd.DataFrame(X[0:11393]), pd.DataFrame(y[0:11393])\n",
        "toxic_samples, toxic_labels, = pd.DataFrame(y[11393:]),pd.DataFrame(y[11393:])\n",
        "\n",
        "# non_toxic_samples\n",
        "# non_tox = non_toxic_samples.merge(non_toxic_labels)\n",
        "# tox = toxic_samples.merge(toxic_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyqqCoJ7objI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(X_test, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3PhiW3i9dJx",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vF-TVr3DJC61",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def map_to_bool(predictions):\n",
        "  \"\"\" Convert array of probabilities to array of boolean values to easily score \"\"\"\n",
        "  # for pred in predictions:\n",
        "  # Map probabilities from continous values to either 1 or 0\n",
        "  for index, prob in enumerate(predictions):\n",
        "    # change likely toxic probability to 1\n",
        "    if prob[0] >= 0.5:\n",
        "      predictions[index] = 1\n",
        "    else:\n",
        "      predictions[index] = 0\n",
        "\n",
        "  return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NFfZtJDobha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score\n",
        "def model_metrics(predictions, y_test):\n",
        "  \"\"\" Create easy to read object containing model evaluation scores \"\"\"\n",
        "  scores = {}\n",
        "  predictions = map_to_bool(predictions)\n",
        "\n",
        "  scores[\"f1_score\"] = f1_score(y_test, predictions)\n",
        "  scores[\"precision_score\"] = precision_score(y_test, predictions)    \n",
        "  scores[\"recall_score\"] = recall_score(y_test, predictions)\n",
        "  scores[\"roc_auc_score\"] = roc_auc_score(y_test, predictions)\n",
        "  \n",
        "  return scores\n",
        "  # LSTM w/ Attn\n",
        "  # print(\"LSTM w/ Attn\")\n",
        "  # print(f\"recall_score: {recall_sc}\")\n",
        "  # print(f\"precision_score: {precision_sc}\")\n",
        "  # print(f\"f1_score: {f1_sc}\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veg5lnAz6Xag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"LSTM w/ Attn\")\n",
        "print(model_metrics(predictions,y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kymmRmB-ITwm",
        "colab_type": "code",
        "outputId": "7ccbb4b0-5853-4f20-8bb7-96fce2e0879a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\"LSTM w/ CNN\"\n",
        "from keras.layers import GlobalMaxPooling1D\n",
        "\n",
        "from sklearn.model_selection import cross_validate\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "\n",
        "# Grid Search model\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "def bi_lstm_cnn():\n",
        "\n",
        "  max_length = 150\n",
        "  vocab_size = embedding_matrix.shape[0]\n",
        "  embedding_size = embedding_matrix.shape[1]\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size,input_length=max_length, weights=[embedding_matrix], trainable=False))\n",
        "  model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
        "  model.add(Conv1D(128, kernel_size=3, padding='valid', kernel_initializer='glorot_uniform'))\n",
        "  model.add(GlobalMaxPooling1D())\n",
        "\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "lstm_cnn_model = KerasClassifier(build_fn=bi_lstm_cnn, epochs=10, batch_size=32, verbose=1)\n",
        "\n",
        "\n",
        "scores = cross_validate(lstm_cnn_model, X, y, cv=10, scoring=['precision', 'f1', 'recall'], return_estimator=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# epochs = [15]\n",
        "# batch_size = [16]\n",
        "\n",
        "# param_grid = dict(epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "# bi_lstm_cnn_model = KerasClassifier(build_fn=bi_lstm_cnn, verbose=1)\n",
        "# grid = GridSearchCV(estimator=bi_lstm_cnn_model, param_grid=param_grid, scoring = ['precision', 'f1', 'recall'], refit='f1', cv=5, verbose=1)\n",
        "# grid_result = grid.fit(X, y)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/10\n",
            "20507/20507 [==============================] - 276s 13ms/step - loss: 0.2659 - acc: 0.8857\n",
            "Epoch 2/10\n",
            "20507/20507 [==============================] - 271s 13ms/step - loss: 0.2020 - acc: 0.9172\n",
            "Epoch 3/10\n",
            "20507/20507 [==============================] - 274s 13ms/step - loss: 0.1609 - acc: 0.9354\n",
            "Epoch 4/10\n",
            "20507/20507 [==============================] - 272s 13ms/step - loss: 0.1174 - acc: 0.9540\n",
            "Epoch 5/10\n",
            "20507/20507 [==============================] - 273s 13ms/step - loss: 0.0716 - acc: 0.9740\n",
            "Epoch 6/10\n",
            "20507/20507 [==============================] - 274s 13ms/step - loss: 0.0363 - acc: 0.9880\n",
            "Epoch 7/10\n",
            "20507/20507 [==============================] - 273s 13ms/step - loss: 0.0225 - acc: 0.9929\n",
            "Epoch 8/10\n",
            "20507/20507 [==============================] - 273s 13ms/step - loss: 0.0119 - acc: 0.9967\n",
            "Epoch 9/10\n",
            "20507/20507 [==============================] - 274s 13ms/step - loss: 0.0149 - acc: 0.9953\n",
            "Epoch 10/10\n",
            "20507/20507 [==============================] - 272s 13ms/step - loss: 0.0128 - acc: 0.9961\n",
            "2279/2279 [==============================] - 12s 5ms/step\n",
            "Epoch 1/10\n",
            "20507/20507 [==============================] - 276s 13ms/step - loss: 0.2659 - acc: 0.8876\n",
            "Epoch 2/10\n",
            "20507/20507 [==============================] - 276s 13ms/step - loss: 0.2018 - acc: 0.9169\n",
            "Epoch 3/10\n",
            "20507/20507 [==============================] - 270s 13ms/step - loss: 0.1617 - acc: 0.9357\n",
            "Epoch 4/10\n",
            "20507/20507 [==============================] - 270s 13ms/step - loss: 0.1162 - acc: 0.9549\n",
            "Epoch 5/10\n",
            "20507/20507 [==============================] - 270s 13ms/step - loss: 0.0695 - acc: 0.9751\n",
            "Epoch 6/10\n",
            "20507/20507 [==============================] - 273s 13ms/step - loss: 0.0338 - acc: 0.9896\n",
            "Epoch 7/10\n",
            "20507/20507 [==============================] - 273s 13ms/step - loss: 0.0177 - acc: 0.9946\n",
            "Epoch 8/10\n",
            "20507/20507 [==============================] - 273s 13ms/step - loss: 0.0138 - acc: 0.9963\n",
            "Epoch 9/10\n",
            "20507/20507 [==============================] - 273s 13ms/step - loss: 0.0096 - acc: 0.9974\n",
            "Epoch 10/10\n",
            "20507/20507 [==============================] - 274s 13ms/step - loss: 0.0141 - acc: 0.9960\n",
            "2279/2279 [==============================] - 12s 5ms/step\n",
            "Epoch 1/10\n",
            "20507/20507 [==============================] - 276s 13ms/step - loss: 0.2607 - acc: 0.8911\n",
            "Epoch 2/10\n",
            "20507/20507 [==============================] - 278s 14ms/step - loss: 0.1996 - acc: 0.9175\n",
            "Epoch 3/10\n",
            "20507/20507 [==============================] - 274s 13ms/step - loss: 0.1587 - acc: 0.9372\n",
            "Epoch 4/10\n",
            "20507/20507 [==============================] - 275s 13ms/step - loss: 0.1097 - acc: 0.9604\n",
            "Epoch 5/10\n",
            "20507/20507 [==============================] - 275s 13ms/step - loss: 0.0636 - acc: 0.9790\n",
            "Epoch 6/10\n",
            "20507/20507 [==============================] - 274s 13ms/step - loss: 0.0349 - acc: 0.9885\n",
            "Epoch 7/10\n",
            "20507/20507 [==============================] - 276s 13ms/step - loss: 0.0201 - acc: 0.9943\n",
            "Epoch 8/10\n",
            "20507/20507 [==============================] - 273s 13ms/step - loss: 0.0139 - acc: 0.9964\n",
            "Epoch 9/10\n",
            "20507/20507 [==============================] - 273s 13ms/step - loss: 0.0172 - acc: 0.9951\n",
            "Epoch 10/10\n",
            "20507/20507 [==============================] - 270s 13ms/step - loss: 0.0135 - acc: 0.9963\n",
            "1312/2279 [================>.............] - ETA: 5s"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Is49eaIeImT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models = scores[\"estimator\"]\n",
        "\n",
        "\n",
        "def save_model(model, split_num, model_name_type):\n",
        "  \"\"\" Save each model used per cross validation split in Google Drive \"\"\"\n",
        "    # saving model\n",
        "    model_filename = f'{model_name_type}_split_{split_num}.h5'\n",
        "    # saving weights\n",
        "    model.save(f\"/content/gdrive/My Drive/Colab Notebooks/{model_filename}\", overwrite=True)\n",
        "\n",
        "\n",
        "for split_num, model in enumerate(models):\n",
        "  # print(model.model)\n",
        "  save_model(model.model, split_num+1, \"lstm_cnn\")\n",
        "\n",
        "\n",
        "# !ls gdrive/My\\ Drive/Colab\\ Notebooks/\n",
        "\n",
        "# def load_model():\n",
        "#     # loading model\n",
        "#     model = model_from_json(open('model_architecture.json').read())\n",
        "#     model.load_weights('model_weights.h5')\n",
        "#     model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "#     return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6adTvpHJsaR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fplaikMSupje",
        "colab_type": "code",
        "outputId": "5af5286f-c274-49ba-d157-0f4b466aa9af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(\"LSTM w/CNN\")\n",
        "predictions = model.predict(X[11000:12000], verbose=1)\n",
        "# print(predictions[0:10])\n",
        "print(model_metrics(predictions,y[11000:12000]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTM w/CNN\n",
            "1000/1000 [==============================] - 6s 6ms/step\n",
            "{'f1_score': 0.8866498740554156, 'precision_score': 0.9041095890410958, 'recall_score': 0.8698517298187809, 'roc_auc_score': 0.8636790455709682}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQC8_I8ZKSw9",
        "colab_type": "code",
        "outputId": "bad96118-461a-4eca-abb3-97fe8290c27d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "from keras.layers import MaxPooling1D, Input, GlobalMaxPool1D, MaxPooling2D, Flatten,TimeDistributed\n",
        "\n",
        "max_length = 150\n",
        "vocab_size = embedding_matrix.shape[0]\n",
        "embedding_size = 300\n",
        "\n",
        "inp = Input(shape=(max_length,))\n",
        "x = Embedding(vocab_size, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
        "x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(1, activation=\"sigmoid\")(x)\n",
        "lstm_model = Model(inputs=inp, outputs=x)\n",
        "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "lstm_model.fit(X_train[0:10], y_train[0:10], batch_size=16, epochs=10, verbose=1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "10/10 [==============================] - 3s 330ms/step - loss: 0.6975 - acc: 0.6000\n",
            "Epoch 2/10\n",
            "10/10 [==============================] - 1s 53ms/step - loss: 0.5202 - acc: 0.9000\n",
            "Epoch 3/10\n",
            "10/10 [==============================] - 1s 53ms/step - loss: 0.4009 - acc: 1.0000\n",
            "Epoch 4/10\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.3422 - acc: 1.0000\n",
            "Epoch 5/10\n",
            "10/10 [==============================] - 1s 55ms/step - loss: 0.2716 - acc: 1.0000\n",
            "Epoch 6/10\n",
            "10/10 [==============================] - 1s 55ms/step - loss: 0.2404 - acc: 1.0000\n",
            "Epoch 7/10\n",
            "10/10 [==============================] - 1s 54ms/step - loss: 0.1907 - acc: 1.0000\n",
            "Epoch 8/10\n",
            "10/10 [==============================] - 1s 54ms/step - loss: 0.1500 - acc: 1.0000\n",
            "Epoch 9/10\n",
            "10/10 [==============================] - 1s 54ms/step - loss: 0.1165 - acc: 1.0000\n",
            "Epoch 10/10\n",
            "10/10 [==============================] - 0s 46ms/step - loss: 0.0789 - acc: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8c40086fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZZPY2D8l2-1",
        "colab_type": "code",
        "outputId": "f679011d-eb6f-43c6-98f5-1b906f275d6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(\"bi-LSTM No max pooling\")\n",
        "predictions = lstm_model.predict(X_test[0:100], verbose=1)\n",
        "# print(predictions)\n",
        "predictions = map_to_bool(predictions)\n",
        "# print(predictions)\n",
        "print(model_metrics(predictions,y_test[0:100]))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bi-LSTM No max pooling\n",
            "100/100 [==============================] - 2s 18ms/step\n",
            "{'f1_score': 0.5777777777777778, 'precision_score': 0.5306122448979592, 'recall_score': 0.6341463414634146, 'roc_auc_score': 0.6221579164944192}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eu3vgJ4vIT3Z",
        "colab_type": "code",
        "outputId": "18c21950-f4b3-4062-d58d-a62351bad9fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        }
      },
      "source": [
        "\"\"\" LSTM MODEL -- With Max Pooling \"\"\"\n",
        "\n",
        "from keras.layers import MaxPooling1D, Input, GlobalMaxPool1D, MaxPooling2D\n",
        "from sklearn.model_selection import cross_validate\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "def create_bi_lstm():\n",
        "\n",
        "  max_length = 150\n",
        "  vocab_size = embedding_matrix.shape[0]\n",
        "  embedding_size = 300\n",
        "\n",
        "  inp = Input(shape=(max_length,))\n",
        "  x = Embedding(vocab_size, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
        "  x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(1, activation=\"sigmoid\")(x)\n",
        "  lstm_model = Model(inputs=inp, outputs=x)\n",
        "  lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return lstm_model\n",
        "\n",
        "\n",
        "bi_lstm_model = KerasClassifier(build_fn=create_bi_lstm, epochs=10, batch_size=32, verbose=1)\n",
        "\n",
        "scores = cross_validate(bi_lstm_model, X, y, cv=10, scoring=['accuracy', 'precision', 'f1', 'recall'])\n",
        "\n",
        "# Grid Search Bi-LSTM model\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "# from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "# epochs = [10, 20]\n",
        "# batch_size = [16, 32]\n",
        "# lstm_cells= [64, 128]\n",
        "# is_trainable = [False, True]\n",
        "\n",
        "# param_grid = dict(epochs=epochs, batch_size=batch_size, lstm_cells=lstm_cells, is_trainable=is_trainable)\n",
        "\n",
        "# bi_lstm_model = KerasClassifier(build_fn=create_bi_lstm, verbose=1)\n",
        "# grid = GridSearchCV(estimator=bi_lstm_model, param_grid=param_grid, cv=10, verbose=1)\n",
        "# grid_result = grid.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "9/9 [==============================] - 3s 381ms/step - loss: 0.8096 - acc: 0.0000e+00\n",
            "Epoch 2/2\n",
            "9/9 [==============================] - 0s 53ms/step - loss: 0.6609 - acc: 0.6667\n",
            "1/1 [==============================] - 1s 1s/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1515: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "9/9 [==============================] - 4s 420ms/step - loss: 0.8847 - acc: 0.0000e+00\n",
            "Epoch 2/2\n",
            "9/9 [==============================] - 0s 55ms/step - loss: 0.7277 - acc: 0.1111\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/2\n",
            "9/9 [==============================] - 4s 460ms/step - loss: 0.6830 - acc: 0.6667\n",
            "Epoch 2/2\n",
            "9/9 [==============================] - 0s 55ms/step - loss: 0.5379 - acc: 1.0000\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Epoch 1/2\n",
            "9/9 [==============================] - 4s 490ms/step - loss: 0.6561 - acc: 0.8889\n",
            "Epoch 2/2\n",
            "9/9 [==============================] - 0s 55ms/step - loss: 0.4596 - acc: 1.0000\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "Epoch 1/2\n",
            "9/9 [==============================] - 5s 526ms/step - loss: 0.5669 - acc: 1.0000\n",
            "Epoch 2/2\n",
            "9/9 [==============================] - 1s 56ms/step - loss: 0.4374 - acc: 1.0000\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "Epoch 1/2\n",
            "9/9 [==============================] - 5s 555ms/step - loss: 0.4793 - acc: 1.0000\n",
            "Epoch 2/2\n",
            "9/9 [==============================] - 0s 55ms/step - loss: 0.3352 - acc: 1.0000\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "Epoch 1/2\n",
            "9/9 [==============================] - 5s 599ms/step - loss: 0.7763 - acc: 0.0000e+00\n",
            "Epoch 2/2\n",
            "9/9 [==============================] - 0s 55ms/step - loss: 0.5727 - acc: 1.0000\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "Epoch 1/2\n",
            "9/9 [==============================] - 6s 624ms/step - loss: 0.6090 - acc: 1.0000\n",
            "Epoch 2/2\n",
            "9/9 [==============================] - 0s 47ms/step - loss: 0.4681 - acc: 1.0000\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "Epoch 1/2\n",
            "9/9 [==============================] - 6s 663ms/step - loss: 0.7213 - acc: 0.2222\n",
            "Epoch 2/2\n",
            "9/9 [==============================] - 0s 55ms/step - loss: 0.5795 - acc: 1.0000\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "Epoch 1/2\n",
            "9/9 [==============================] - 6s 690ms/step - loss: 0.7889 - acc: 0.0000e+00\n",
            "Epoch 2/2\n",
            "9/9 [==============================] - 1s 65ms/step - loss: 0.6220 - acc: 1.0000\n",
            "1/1 [==============================] - 2s 2s/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ju8VKOB_c5WK",
        "colab_type": "code",
        "outputId": "42ea0ab2-b208-45e0-9f9c-56e2c06aba45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.layers import Input, Flatten, Activation, multiply, TimeDistributed, RepeatVector, Permute, Lambda\n",
        "from keras import backend as K\n",
        "\n",
        "from sklearn.model_selection import cross_validate\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "\"\"\"\n",
        "  Bi-LSTM w/ Attn and K-fold cross validation\n",
        "\"\"\"\n",
        "\n",
        "def create_attn_lstm():\n",
        "\n",
        "\n",
        "  units = 32\n",
        "  max_length = 150\n",
        "  vocab_size = embedding_matrix.shape[0]\n",
        "  embedding_size = embedding_matrix.shape[1]\n",
        "\n",
        "\n",
        "  _input = Input(shape=[max_length], dtype='int32')\n",
        "\n",
        "  # get the embedding layer\n",
        "  embedded = Embedding(\n",
        "          input_dim=vocab_size,\n",
        "          output_dim=embedding_size,\n",
        "          input_length=max_length,\n",
        "          trainable=False,\n",
        "          weights=[embedding_matrix]\n",
        "      )(_input)\n",
        "\n",
        "  # Bidirectional(LSTM(128, return_sequences=True))\n",
        "  # activations = LSTM(units, return_sequences=True)(embedded)\n",
        "  # Bidirectional(LSTM(64, return_sequences=True))\n",
        "  activations = Bidirectional(LSTM(32, return_sequences=True))(embedded)\n",
        "\n",
        "\n",
        "  # compute importance for each step\n",
        "  attention = TimeDistributed(Dense(1, activation='tanh'))(activations) \n",
        "  attention = Flatten()(attention)\n",
        "  attention = Activation('softmax')(attention)\n",
        "  # adjusting vector shapes for multiplication\n",
        "  attention = RepeatVector(64)(attention)\n",
        "  attention = Permute([2, 1])(attention)\n",
        "\n",
        "  # apply the attention\n",
        "  sent_representation = multiply([activations, attention])\n",
        "\n",
        "  sent_representation = Lambda(lambda xin: K.sum(xin, axis=1))(sent_representation)\n",
        "\n",
        "  probabilities = Dense(1, activation='sigmoid')(sent_representation)\n",
        "\n",
        "  model = Model(input=_input, output=probabilities)\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "attn_lstm_model = KerasClassifier(build_fn=create_attn_lstm, epochs=10, batch_size=32, verbose=1)\n",
        "\n",
        "scores = cross_validate(attn_lstm_model, X, y, cv=10, scoring=['precision', 'f1', 'recall'],return_estimator=True)\n",
        "\n",
        "\n",
        "print(scores)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/10\n",
            "20507/20507 [==============================] - 371s 18ms/step - loss: 0.3483 - acc: 0.8481\n",
            "Epoch 2/10\n",
            "20507/20507 [==============================] - 367s 18ms/step - loss: 0.2546 - acc: 0.8947\n",
            "Epoch 3/10\n",
            "20507/20507 [==============================] - 372s 18ms/step - loss: 0.2273 - acc: 0.9073\n",
            "Epoch 4/10\n",
            "20507/20507 [==============================] - 372s 18ms/step - loss: 0.2062 - acc: 0.9166\n",
            "Epoch 5/10\n",
            "20507/20507 [==============================] - 370s 18ms/step - loss: 0.1969 - acc: 0.9204\n",
            "Epoch 6/10\n",
            "20507/20507 [==============================] - 368s 18ms/step - loss: 0.1893 - acc: 0.9257\n",
            "Epoch 7/10\n",
            "20507/20507 [==============================] - 374s 18ms/step - loss: 0.1674 - acc: 0.9352\n",
            "Epoch 8/10\n",
            "20507/20507 [==============================] - 367s 18ms/step - loss: 0.1477 - acc: 0.9434\n",
            "Epoch 9/10\n",
            "20507/20507 [==============================] - 371s 18ms/step - loss: 0.1305 - acc: 0.9508\n",
            "Epoch 10/10\n",
            "20507/20507 [==============================] - 367s 18ms/step - loss: 0.1152 - acc: 0.9582\n",
            "2279/2279 [==============================] - 16s 7ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "20507/20507 [==============================] - 374s 18ms/step - loss: 0.3432 - acc: 0.8540\n",
            "Epoch 2/10\n",
            "20507/20507 [==============================] - 375s 18ms/step - loss: 0.2702 - acc: 0.8869\n",
            "Epoch 3/10\n",
            "20507/20507 [==============================] - 370s 18ms/step - loss: 0.2443 - acc: 0.8997\n",
            "Epoch 4/10\n",
            "20507/20507 [==============================] - 375s 18ms/step - loss: 0.2232 - acc: 0.9081\n",
            "Epoch 5/10\n",
            "20507/20507 [==============================] - 371s 18ms/step - loss: 0.2109 - acc: 0.9154\n",
            "Epoch 6/10\n",
            "20507/20507 [==============================] - 376s 18ms/step - loss: 0.1849 - acc: 0.9252\n",
            "Epoch 7/10\n",
            "20507/20507 [==============================] - 375s 18ms/step - loss: 0.1673 - acc: 0.9358\n",
            "Epoch 8/10\n",
            "20507/20507 [==============================] - 367s 18ms/step - loss: 0.1490 - acc: 0.9412\n",
            "Epoch 9/10\n",
            "20507/20507 [==============================] - 372s 18ms/step - loss: 0.1321 - acc: 0.9483\n",
            "Epoch 10/10\n",
            "20507/20507 [==============================] - 370s 18ms/step - loss: 0.1176 - acc: 0.9552\n",
            "2279/2279 [==============================] - 16s 7ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "20507/20507 [==============================] - 372s 18ms/step - loss: 0.3558 - acc: 0.8473\n",
            "Epoch 2/10\n",
            "20507/20507 [==============================] - 358s 17ms/step - loss: 0.2814 - acc: 0.8851\n",
            "Epoch 3/10\n",
            "20507/20507 [==============================] - 357s 17ms/step - loss: 0.2403 - acc: 0.9020\n",
            "Epoch 4/10\n",
            "20507/20507 [==============================] - 349s 17ms/step - loss: 0.2228 - acc: 0.9088\n",
            "Epoch 5/10\n",
            "20507/20507 [==============================] - 347s 17ms/step - loss: 0.1991 - acc: 0.9196\n",
            "Epoch 6/10\n",
            "20507/20507 [==============================] - 352s 17ms/step - loss: 0.1835 - acc: 0.9278\n",
            "Epoch 7/10\n",
            "20507/20507 [==============================] - 358s 17ms/step - loss: 0.1643 - acc: 0.9367\n",
            "Epoch 8/10\n",
            "20507/20507 [==============================] - 353s 17ms/step - loss: 0.1448 - acc: 0.9448\n",
            "Epoch 9/10\n",
            "20507/20507 [==============================] - 353s 17ms/step - loss: 0.1302 - acc: 0.9520\n",
            "Epoch 10/10\n",
            "20507/20507 [==============================] - 343s 17ms/step - loss: 0.1187 - acc: 0.9561\n",
            "2279/2279 [==============================] - 15s 7ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "20507/20507 [==============================] - 344s 17ms/step - loss: 0.3601 - acc: 0.8404\n",
            "Epoch 2/10\n",
            "20507/20507 [==============================] - 353s 17ms/step - loss: 0.2640 - acc: 0.8909\n",
            "Epoch 3/10\n",
            "20507/20507 [==============================] - 346s 17ms/step - loss: 0.2368 - acc: 0.9068\n",
            "Epoch 4/10\n",
            "20507/20507 [==============================] - 344s 17ms/step - loss: 0.2128 - acc: 0.9138\n",
            "Epoch 5/10\n",
            "20507/20507 [==============================] - 352s 17ms/step - loss: 0.1948 - acc: 0.9237\n",
            "Epoch 6/10\n",
            "20507/20507 [==============================] - 349s 17ms/step - loss: 0.1782 - acc: 0.9293\n",
            "Epoch 7/10\n",
            "20507/20507 [==============================] - 350s 17ms/step - loss: 0.1683 - acc: 0.9338\n",
            "Epoch 8/10\n",
            "20507/20507 [==============================] - 341s 17ms/step - loss: 0.1467 - acc: 0.9450\n",
            "Epoch 9/10\n",
            "20507/20507 [==============================] - 353s 17ms/step - loss: 0.1302 - acc: 0.9513\n",
            "Epoch 10/10\n",
            "20507/20507 [==============================] - 349s 17ms/step - loss: 0.1160 - acc: 0.9575\n",
            "2279/2279 [==============================] - 15s 7ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "20507/20507 [==============================] - 366s 18ms/step - loss: 0.3670 - acc: 0.8356\n",
            "Epoch 2/10\n",
            "20507/20507 [==============================] - 376s 18ms/step - loss: 0.2696 - acc: 0.8874\n",
            "Epoch 3/10\n",
            "20507/20507 [==============================] - 362s 18ms/step - loss: 0.2493 - acc: 0.8956\n",
            "Epoch 4/10\n",
            "20507/20507 [==============================] - 374s 18ms/step - loss: 0.2653 - acc: 0.8949\n",
            "Epoch 5/10\n",
            "20507/20507 [==============================] - 372s 18ms/step - loss: 0.2343 - acc: 0.9070\n",
            "Epoch 6/10\n",
            "20507/20507 [==============================] - 369s 18ms/step - loss: 0.2067 - acc: 0.9182\n",
            "Epoch 7/10\n",
            "20507/20507 [==============================] - 366s 18ms/step - loss: 0.1800 - acc: 0.9307\n",
            "Epoch 8/10\n",
            "20507/20507 [==============================] - 366s 18ms/step - loss: 0.1612 - acc: 0.9389\n",
            "Epoch 9/10\n",
            "20507/20507 [==============================] - 361s 18ms/step - loss: 0.1490 - acc: 0.9429\n",
            "Epoch 10/10\n",
            "20507/20507 [==============================] - 345s 17ms/step - loss: 0.1286 - acc: 0.9519\n",
            "2279/2279 [==============================] - 16s 7ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "20507/20507 [==============================] - 354s 17ms/step - loss: 0.3483 - acc: 0.8486\n",
            "Epoch 2/10\n",
            "20507/20507 [==============================] - 343s 17ms/step - loss: 0.2602 - acc: 0.8920\n",
            "Epoch 3/10\n",
            "20507/20507 [==============================] - 351s 17ms/step - loss: 0.2348 - acc: 0.9032\n",
            "Epoch 4/10\n",
            "20507/20507 [==============================] - 361s 18ms/step - loss: 0.2249 - acc: 0.9071\n",
            "Epoch 5/10\n",
            "20507/20507 [==============================] - 361s 18ms/step - loss: 0.1949 - acc: 0.9219\n",
            "Epoch 6/10\n",
            "20507/20507 [==============================] - 362s 18ms/step - loss: 0.1803 - acc: 0.9290\n",
            "Epoch 7/10\n",
            "20507/20507 [==============================] - 364s 18ms/step - loss: 0.1618 - acc: 0.9367\n",
            "Epoch 8/10\n",
            "20507/20507 [==============================] - 359s 18ms/step - loss: 0.1465 - acc: 0.9429\n",
            "Epoch 9/10\n",
            "20507/20507 [==============================] - 364s 18ms/step - loss: 0.1314 - acc: 0.9501\n",
            "Epoch 10/10\n",
            "20507/20507 [==============================] - 351s 17ms/step - loss: 0.1182 - acc: 0.9569\n",
            "2279/2279 [==============================] - 16s 7ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "20508/20508 [==============================] - 357s 17ms/step - loss: 0.3470 - acc: 0.8524\n",
            "Epoch 2/10\n",
            "20508/20508 [==============================] - 358s 17ms/step - loss: 0.2602 - acc: 0.8946\n",
            "Epoch 3/10\n",
            "20508/20508 [==============================] - 355s 17ms/step - loss: 0.2401 - acc: 0.9030\n",
            "Epoch 4/10\n",
            "20508/20508 [==============================] - 357s 17ms/step - loss: 0.2200 - acc: 0.9128\n",
            "Epoch 5/10\n",
            "20508/20508 [==============================] - 352s 17ms/step - loss: 0.2074 - acc: 0.9195\n",
            "Epoch 6/10\n",
            "20508/20508 [==============================] - 359s 17ms/step - loss: 0.1844 - acc: 0.9269\n",
            "Epoch 7/10\n",
            "20508/20508 [==============================] - 360s 18ms/step - loss: 0.1763 - acc: 0.9303\n",
            "Epoch 8/10\n",
            "20508/20508 [==============================] - 363s 18ms/step - loss: 0.1631 - acc: 0.9350\n",
            "Epoch 9/10\n",
            "20508/20508 [==============================] - 362s 18ms/step - loss: 0.1449 - acc: 0.9437\n",
            "Epoch 10/10\n",
            "20508/20508 [==============================] - 364s 18ms/step - loss: 0.1393 - acc: 0.9473\n",
            "2278/2278 [==============================] - 16s 7ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "20508/20508 [==============================] - 368s 18ms/step - loss: 0.3475 - acc: 0.8506\n",
            "Epoch 2/10\n",
            "20508/20508 [==============================] - 357s 17ms/step - loss: 0.2863 - acc: 0.8811\n",
            "Epoch 3/10\n",
            "20508/20508 [==============================] - 351s 17ms/step - loss: 0.2447 - acc: 0.9014\n",
            "Epoch 4/10\n",
            "20508/20508 [==============================] - 362s 18ms/step - loss: 0.2196 - acc: 0.9116\n",
            "Epoch 5/10\n",
            "20508/20508 [==============================] - 347s 17ms/step - loss: 0.2007 - acc: 0.9208\n",
            "Epoch 6/10\n",
            "20508/20508 [==============================] - 355s 17ms/step - loss: 0.1849 - acc: 0.9272\n",
            "Epoch 7/10\n",
            "20508/20508 [==============================] - 361s 18ms/step - loss: 0.1649 - acc: 0.9367\n",
            "Epoch 8/10\n",
            "20508/20508 [==============================] - 351s 17ms/step - loss: 0.1494 - acc: 0.9441\n",
            "Epoch 9/10\n",
            "20508/20508 [==============================] - 347s 17ms/step - loss: 0.1317 - acc: 0.9514\n",
            "Epoch 10/10\n",
            "20508/20508 [==============================] - 346s 17ms/step - loss: 0.1210 - acc: 0.9568\n",
            "2278/2278 [==============================] - 16s 7ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "20508/20508 [==============================] - 337s 16ms/step - loss: 0.3327 - acc: 0.8541\n",
            "Epoch 2/10\n",
            "20508/20508 [==============================] - 332s 16ms/step - loss: 0.2421 - acc: 0.9016\n",
            "Epoch 3/10\n",
            "20508/20508 [==============================] - 338s 16ms/step - loss: 0.2244 - acc: 0.9094\n",
            "Epoch 4/10\n",
            "20508/20508 [==============================] - 331s 16ms/step - loss: 0.2217 - acc: 0.9075\n",
            "Epoch 5/10\n",
            "20508/20508 [==============================] - 328s 16ms/step - loss: 0.1949 - acc: 0.9212\n",
            "Epoch 6/10\n",
            "20508/20508 [==============================] - 333s 16ms/step - loss: 0.1806 - acc: 0.9287\n",
            "Epoch 7/10\n",
            "20508/20508 [==============================] - 333s 16ms/step - loss: 0.1762 - acc: 0.9300\n",
            "Epoch 8/10\n",
            "20508/20508 [==============================] - 336s 16ms/step - loss: 0.1821 - acc: 0.9274\n",
            "Epoch 9/10\n",
            "20508/20508 [==============================] - 336s 16ms/step - loss: 0.1538 - acc: 0.9393\n",
            "Epoch 10/10\n",
            "20508/20508 [==============================] - 337s 16ms/step - loss: 0.1485 - acc: 0.9424\n",
            "2278/2278 [==============================] - 16s 7ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "20508/20508 [==============================] - 343s 17ms/step - loss: 0.3513 - acc: 0.8457\n",
            "Epoch 2/10\n",
            "20508/20508 [==============================] - 342s 17ms/step - loss: 0.2623 - acc: 0.8930\n",
            "Epoch 3/10\n",
            "20508/20508 [==============================] - 345s 17ms/step - loss: 0.2380 - acc: 0.9033\n",
            "Epoch 4/10\n",
            "20508/20508 [==============================] - 344s 17ms/step - loss: 0.2192 - acc: 0.9095\n",
            "Epoch 5/10\n",
            "20508/20508 [==============================] - 342s 17ms/step - loss: 0.2088 - acc: 0.9137\n",
            "Epoch 6/10\n",
            "20508/20508 [==============================] - 343s 17ms/step - loss: 0.1990 - acc: 0.9200\n",
            "Epoch 7/10\n",
            "20508/20508 [==============================] - 346s 17ms/step - loss: 0.1733 - acc: 0.9309\n",
            "Epoch 8/10\n",
            "20508/20508 [==============================] - 345s 17ms/step - loss: 0.1536 - acc: 0.9391\n",
            "Epoch 9/10\n",
            "20508/20508 [==============================] - 346s 17ms/step - loss: 0.1345 - acc: 0.9487\n",
            "Epoch 10/10\n",
            "20508/20508 [==============================] - 354s 17ms/step - loss: 0.1191 - acc: 0.9551\n",
            "2278/2278 [==============================] - 16s 7ms/step\n",
            "{'fit_time': array([3708.84658384, 3726.05139613, 3543.84699178, 3483.19900584,\n",
            "       3660.57607913, 3574.0513525 , 3591.19635582, 3549.49508977,\n",
            "       3345.90019512, 3454.98472714]), 'score_time': array([15.72144794, 16.10753679, 15.2702179 , 15.34935832, 15.85596752,\n",
            "       15.92558622, 16.40870833, 16.05354333, 15.84623599, 16.20684052]), 'estimator': (<keras.wrappers.scikit_learn.KerasClassifier object at 0x7f39b572f550>, <keras.wrappers.scikit_learn.KerasClassifier object at 0x7f39b572fb38>, <keras.wrappers.scikit_learn.KerasClassifier object at 0x7f393fbb2470>, <keras.wrappers.scikit_learn.KerasClassifier object at 0x7f393fbabf28>, <keras.wrappers.scikit_learn.KerasClassifier object at 0x7f393f46fcf8>, <keras.wrappers.scikit_learn.KerasClassifier object at 0x7f393ed0d7f0>, <keras.wrappers.scikit_learn.KerasClassifier object at 0x7f393ed84f60>, <keras.wrappers.scikit_learn.KerasClassifier object at 0x7f393e648278>, <keras.wrappers.scikit_learn.KerasClassifier object at 0x7f393ed13ef0>, <keras.wrappers.scikit_learn.KerasClassifier object at 0x7f38ed7f4198>), 'test_precision': array([0.88405797, 0.90750436, 0.92056487, 0.91688539, 0.90391459,\n",
            "       0.90802525, 0.92197907, 0.89276373, 0.90026019, 0.90228873]), 'test_f1': array([0.89127632, 0.9039548 , 0.90774587, 0.91170074, 0.90110865,\n",
            "       0.9142079 , 0.88412409, 0.89942907, 0.90615452, 0.90507726]), 'test_recall': array([0.89861352, 0.9004329 , 0.89527897, 0.90657439, 0.89832007,\n",
            "       0.92047532, 0.84925504, 0.90619469, 0.91212654, 0.90788308])}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLcjwoL2HRD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "models = scores[\"estimator\"]\n",
        "\n",
        "\n",
        "def save_model(model, split_num, model_name_type):\n",
        "    # saving model\n",
        "    model_filename = f'{model_name_type}_split_{split_num}.h5'\n",
        "    # saving weights\n",
        "    model.save(f\"/content/gdrive/My Drive/Colab Notebooks/{model_filename}\", overwrite=True)\n",
        "\n",
        "\n",
        "for split_num, model in enumerate(models):\n",
        "  # print(model.model)\n",
        "  save_model(model.model, split_num+1, \"lstm_attn\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1-QHGc5IT8E",
        "colab_type": "code",
        "outputId": "b2142e75-8f74-4c81-dad8-352f014754fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "source": [
        "print(\"LSTM with Max Pooling:\")\n",
        "predictions = lstm_model.predict(X_test, verbose=1)\n",
        "print(model_metrics(predictions,y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTM with Max Pooling:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-248e82d5619c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LSTM with Max Pooling:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m         \u001b[0;31m# Case 2: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1380\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    139\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_7 to have shape (150,) but got array with shape (1,)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QD-Ll_GyDuD",
        "colab_type": "code",
        "outputId": "3d00fb82-e8fe-4198-97c7-8afcad7d2157",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\"\"\" SIMPLE LSTM -- Using as baseline \"\"\"\n",
        "\n",
        "\"\"\" Dimensional issue with Bidirectional LSTM, Needs GlobalMaxPooling after Bi-LSTM layer \"\"\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# From Farhad:\n",
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=True)\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "l_lstm = Bidirectional(LSTM(100, recurrent_dropout=0.1))(embedded_sequences)\n",
        "preds = Dense(len(macronum), activation='softmax')(l_lstm)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# inp = Input(shape=(max_length,))\n",
        "# x = Embedding(vocab_size, embed_size, weights=[embedding_matrix])(inp)\n",
        "# x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
        "# # x = GlobalMaxPool1D()(x)\n",
        "# x = Dense(2, activation=\"sigmoid\")(x)\n",
        "# lstm_model = Model(inputs=inp, outputs=x)\n",
        "# lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "  FROM Medium: https://towardsdatascience.com/sentence-classification-using-bi-lstm-b74151ffa565\n",
        "\n",
        "  # main model\n",
        "  input = Input(shape=(max_len,))\n",
        "  model = Embedding(vocab_size,100,weights=[embedding_matrix],input_length=max_len)(input)\n",
        "  model =  Bidirectional (LSTM (100,return_sequences=True,dropout=0.50),merge_mode='concat')(model)\n",
        "  model = TimeDistributed(Dense(100,activation='relu'))(model)\n",
        "  model = Flatten()(model)\n",
        "  model = Dense(100,activation='relu')(model)\n",
        "  output = Dense(3,activation='softmax')(model)\n",
        "  model = Model(input,output)\n",
        "  model.compile(loss='sparse_categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\"Simple LSTM\"\n",
        "from keras.layers import GlobalMaxPooling1D\n",
        "\n",
        "from sklearn.model_selection import cross_validate\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "\n",
        "# Grid Search model\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "def create_lstm():\n",
        "\n",
        "  max_length = 150\n",
        "  vocab_size = embedding_matrix.shape[0]\n",
        "  embedding_size = 300\n",
        "\n",
        "\n",
        "  simple_lstm = Sequential()\n",
        "  simple_lstm.add(Embedding(vocab_size, embed_size, weights=[embedding_matrix], trainable=False))\n",
        "  simple_lstm.add(LSTM(128))\n",
        "  simple_lstm.add(Dense(1, activation='sigmoid'))\n",
        "  simple_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return simple_lstm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "simple_lstm = KerasClassifier(build_fn=create_lstm, epochs=10, batch_size=32, verbose=1)\n",
        "\n",
        "\n",
        "scores = cross_validate(simple_lstm, X, y, cv=10, scoring=['precision', 'f1', 'recall'], return_estimator=True)\n",
        "\n",
        "\n",
        "scores\n",
        "\n",
        "\n",
        "# batch_size = 16\n",
        "# simple_lstm.fit(X_train, y_train, batch_size=batch_size, epochs=10, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/10\n",
            "20507/20507 [==============================] - 114s 6ms/step - loss: 0.3131 - acc: 0.8678\n",
            "Epoch 2/10\n",
            "20507/20507 [==============================] - 110s 5ms/step - loss: 0.2303 - acc: 0.9048\n",
            "Epoch 3/10\n",
            "20507/20507 [==============================] - 109s 5ms/step - loss: 0.1977 - acc: 0.9207\n",
            "Epoch 4/10\n",
            "20507/20507 [==============================] - 108s 5ms/step - loss: 0.1681 - acc: 0.9327\n",
            "Epoch 5/10\n",
            "20507/20507 [==============================] - 109s 5ms/step - loss: 0.1366 - acc: 0.9453\n",
            "Epoch 6/10\n",
            "20507/20507 [==============================] - 109s 5ms/step - loss: 0.1067 - acc: 0.9607\n",
            "Epoch 7/10\n",
            "20507/20507 [==============================] - 108s 5ms/step - loss: 0.1051 - acc: 0.9600\n",
            "Epoch 8/10\n",
            "20507/20507 [==============================] - 109s 5ms/step - loss: 0.0616 - acc: 0.9787\n",
            "Epoch 9/10\n",
            "20507/20507 [==============================] - 108s 5ms/step - loss: 0.0401 - acc: 0.9867\n",
            "Epoch 10/10\n",
            "20507/20507 [==============================] - 108s 5ms/step - loss: 0.0257 - acc: 0.9921\n",
            "2279/2279 [==============================] - 2s 1ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "20507/20507 [==============================] - 110s 5ms/step - loss: 0.3084 - acc: 0.8689\n",
            "Epoch 2/10\n",
            "20507/20507 [==============================] - 109s 5ms/step - loss: 0.2467 - acc: 0.8985\n",
            "Epoch 3/10\n",
            "20507/20507 [==============================] - 109s 5ms/step - loss: 0.2043 - acc: 0.9145\n",
            "Epoch 4/10\n",
            "20507/20507 [==============================] - 109s 5ms/step - loss: 0.1776 - acc: 0.9276\n",
            "Epoch 5/10\n",
            "20507/20507 [==============================] - 108s 5ms/step - loss: 0.1525 - acc: 0.9396\n",
            "Epoch 6/10\n",
            "20507/20507 [==============================] - 109s 5ms/step - loss: 0.1198 - acc: 0.9531\n",
            "Epoch 7/10\n",
            "20507/20507 [==============================] - 109s 5ms/step - loss: 0.0911 - acc: 0.9656\n",
            "Epoch 8/10\n",
            "20507/20507 [==============================] - 107s 5ms/step - loss: 0.0602 - acc: 0.9784\n",
            "Epoch 9/10\n",
            "20507/20507 [==============================] - 109s 5ms/step - loss: 0.0403 - acc: 0.9869\n",
            "Epoch 10/10\n",
            "20507/20507 [==============================] - 109s 5ms/step - loss: 0.0264 - acc: 0.9921\n",
            "2279/2279 [==============================] - 2s 1ms/step\n",
            "Epoch 1/10\n",
            "20507/20507 [==============================] - 108s 5ms/step - loss: 0.3126 - acc: 0.8671\n",
            "Epoch 2/10\n",
            "20507/20507 [==============================] - 110s 5ms/step - loss: 0.2321 - acc: 0.9070\n",
            "Epoch 3/10\n",
            "20507/20507 [==============================] - 107s 5ms/step - loss: 0.2003 - acc: 0.9176\n",
            "Epoch 4/10\n",
            "20507/20507 [==============================] - 107s 5ms/step - loss: 0.1650 - acc: 0.9335\n",
            "Epoch 5/10\n",
            "20507/20507 [==============================] - 108s 5ms/step - loss: 0.1365 - acc: 0.9470\n",
            "Epoch 6/10\n",
            "20507/20507 [==============================] - 108s 5ms/step - loss: 0.1068 - acc: 0.9587\n",
            "Epoch 7/10\n",
            "20507/20507 [==============================] - 108s 5ms/step - loss: 0.0784 - acc: 0.9713\n",
            "Epoch 8/10\n",
            "20507/20507 [==============================] - 109s 5ms/step - loss: 0.0582 - acc: 0.9798\n",
            "Epoch 9/10\n",
            "20507/20507 [==============================] - 108s 5ms/step - loss: 0.0364 - acc: 0.9882\n",
            "Epoch 10/10\n",
            "20507/20507 [==============================] - 109s 5ms/step - loss: 0.0205 - acc: 0.9939\n",
            "2279/2279 [==============================] - 3s 1ms/step\n",
            "Epoch 1/10\n",
            "20507/20507 [==============================] - 109s 5ms/step - loss: 0.3096 - acc: 0.8674\n",
            "Epoch 2/10\n",
            "20507/20507 [==============================] - 108s 5ms/step - loss: 0.2325 - acc: 0.9050\n",
            "Epoch 3/10\n",
            "20507/20507 [==============================] - 109s 5ms/step - loss: 0.1962 - acc: 0.9185\n",
            "Epoch 4/10\n",
            "20507/20507 [==============================] - 108s 5ms/step - loss: 0.1671 - acc: 0.9327\n",
            "Epoch 5/10\n",
            "20507/20507 [==============================] - 108s 5ms/step - loss: 0.1352 - acc: 0.9458\n",
            "Epoch 6/10\n",
            "20507/20507 [==============================] - 109s 5ms/step - loss: 0.1070 - acc: 0.9588\n",
            "Epoch 7/10\n",
            "20507/20507 [==============================] - 109s 5ms/step - loss: 0.0763 - acc: 0.9732\n",
            "Epoch 8/10\n",
            "20507/20507 [==============================] - 109s 5ms/step - loss: 0.0521 - acc: 0.9822\n",
            "Epoch 9/10\n",
            "20507/20507 [==============================] - 109s 5ms/step - loss: 0.0359 - acc: 0.9886\n",
            "Epoch 10/10\n",
            "20507/20507 [==============================] - 108s 5ms/step - loss: 0.0242 - acc: 0.9928\n",
            "2279/2279 [==============================] - 3s 1ms/step\n",
            "Epoch 1/10\n",
            "20507/20507 [==============================] - 112s 5ms/step - loss: 0.3080 - acc: 0.8698\n",
            "Epoch 2/10\n",
            "20507/20507 [==============================] - 112s 5ms/step - loss: 0.2300 - acc: 0.9048\n",
            "Epoch 3/10\n",
            "20507/20507 [==============================] - 109s 5ms/step - loss: 0.1948 - acc: 0.9202\n",
            "Epoch 4/10\n",
            "20507/20507 [==============================] - 110s 5ms/step - loss: 0.1665 - acc: 0.9340\n",
            "Epoch 5/10\n",
            "20507/20507 [==============================] - 110s 5ms/step - loss: 0.1383 - acc: 0.9466\n",
            "Epoch 6/10\n",
            "20507/20507 [==============================] - 108s 5ms/step - loss: 0.1721 - acc: 0.9327\n",
            "Epoch 7/10\n",
            "20507/20507 [==============================] - 113s 6ms/step - loss: 0.1182 - acc: 0.9554\n",
            "Epoch 8/10\n",
            "20507/20507 [==============================] - 115s 6ms/step - loss: 0.0763 - acc: 0.9733\n",
            "Epoch 9/10\n",
            "20507/20507 [==============================] - 109s 5ms/step - loss: 0.0482 - acc: 0.9850\n",
            "Epoch 10/10\n",
            "20507/20507 [==============================] - 111s 5ms/step - loss: 0.0304 - acc: 0.9910\n",
            "2279/2279 [==============================] - 3s 1ms/step\n",
            "Epoch 1/10\n",
            "20507/20507 [==============================] - 117s 6ms/step - loss: 0.3141 - acc: 0.8661\n",
            "Epoch 2/10\n",
            "20507/20507 [==============================] - 115s 6ms/step - loss: 0.2308 - acc: 0.9045\n",
            "Epoch 3/10\n",
            "20507/20507 [==============================] - 113s 5ms/step - loss: 0.2015 - acc: 0.9179\n",
            "Epoch 4/10\n",
            "20507/20507 [==============================] - 114s 6ms/step - loss: 0.1739 - acc: 0.9307\n",
            "Epoch 5/10\n",
            "20507/20507 [==============================] - 117s 6ms/step - loss: 0.1467 - acc: 0.9416\n",
            "Epoch 6/10\n",
            "20507/20507 [==============================] - 116s 6ms/step - loss: 0.1494 - acc: 0.9395\n",
            "Epoch 7/10\n",
            "20507/20507 [==============================] - 110s 5ms/step - loss: 0.1009 - acc: 0.9616\n",
            "Epoch 8/10\n",
            "20507/20507 [==============================] - 109s 5ms/step - loss: 0.0701 - acc: 0.9754\n",
            "Epoch 9/10\n",
            "20507/20507 [==============================] - 108s 5ms/step - loss: 0.0497 - acc: 0.9832\n",
            "Epoch 10/10\n",
            "20507/20507 [==============================] - 108s 5ms/step - loss: 0.0326 - acc: 0.9901\n",
            "2279/2279 [==============================] - 3s 1ms/step\n",
            "Epoch 1/10\n",
            "20508/20508 [==============================] - 112s 5ms/step - loss: 0.3246 - acc: 0.8675\n",
            "Epoch 2/10\n",
            "20508/20508 [==============================] - 110s 5ms/step - loss: 0.2328 - acc: 0.9061\n",
            "Epoch 3/10\n",
            "20508/20508 [==============================] - 110s 5ms/step - loss: 0.2019 - acc: 0.9173\n",
            "Epoch 4/10\n",
            "20508/20508 [==============================] - 111s 5ms/step - loss: 0.1698 - acc: 0.9318\n",
            "Epoch 5/10\n",
            "20508/20508 [==============================] - 110s 5ms/step - loss: 0.1427 - acc: 0.9430\n",
            "Epoch 6/10\n",
            "20508/20508 [==============================] - 110s 5ms/step - loss: 0.1110 - acc: 0.9570\n",
            "Epoch 7/10\n",
            "20508/20508 [==============================] - 110s 5ms/step - loss: 0.0827 - acc: 0.9692\n",
            "Epoch 8/10\n",
            "20508/20508 [==============================] - 109s 5ms/step - loss: 0.0540 - acc: 0.9815\n",
            "Epoch 9/10\n",
            "20508/20508 [==============================] - 109s 5ms/step - loss: 0.0373 - acc: 0.9878\n",
            "Epoch 10/10\n",
            "20508/20508 [==============================] - 110s 5ms/step - loss: 0.0254 - acc: 0.9928\n",
            "2278/2278 [==============================] - 3s 1ms/step\n",
            "Epoch 1/10\n",
            "20508/20508 [==============================] - 111s 5ms/step - loss: 0.3246 - acc: 0.8648\n",
            "Epoch 2/10\n",
            "20508/20508 [==============================] - 110s 5ms/step - loss: 0.2298 - acc: 0.9052\n",
            "Epoch 3/10\n",
            "20508/20508 [==============================] - 111s 5ms/step - loss: 0.1963 - acc: 0.9178\n",
            "Epoch 4/10\n",
            "20508/20508 [==============================] - 110s 5ms/step - loss: 0.1686 - acc: 0.9315\n",
            "Epoch 5/10\n",
            "20508/20508 [==============================] - 110s 5ms/step - loss: 0.1421 - acc: 0.9425\n",
            "Epoch 6/10\n",
            "20508/20508 [==============================] - 110s 5ms/step - loss: 0.1059 - acc: 0.9593\n",
            "Epoch 7/10\n",
            "20508/20508 [==============================] - 110s 5ms/step - loss: 0.0776 - acc: 0.9721\n",
            "Epoch 8/10\n",
            "20508/20508 [==============================] - 110s 5ms/step - loss: 0.0507 - acc: 0.9836\n",
            "Epoch 9/10\n",
            "20508/20508 [==============================] - 109s 5ms/step - loss: 0.0322 - acc: 0.9899\n",
            "Epoch 10/10\n",
            "20508/20508 [==============================] - 110s 5ms/step - loss: 0.0218 - acc: 0.9936\n",
            "2278/2278 [==============================] - 3s 1ms/step\n",
            "Epoch 1/10\n",
            "20508/20508 [==============================] - 109s 5ms/step - loss: 0.3087 - acc: 0.8732\n",
            "Epoch 2/10\n",
            "20508/20508 [==============================] - 107s 5ms/step - loss: 0.2276 - acc: 0.9054\n",
            "Epoch 3/10\n",
            "20508/20508 [==============================] - 107s 5ms/step - loss: 0.1946 - acc: 0.9207\n",
            "Epoch 4/10\n",
            "20508/20508 [==============================] - 107s 5ms/step - loss: 0.1647 - acc: 0.9329\n",
            "Epoch 5/10\n",
            "20508/20508 [==============================] - 106s 5ms/step - loss: 0.1337 - acc: 0.9462\n",
            "Epoch 6/10\n",
            "20508/20508 [==============================] - 107s 5ms/step - loss: 0.1067 - acc: 0.9591\n",
            "Epoch 7/10\n",
            "20508/20508 [==============================] - 107s 5ms/step - loss: 0.0756 - acc: 0.9727\n",
            "Epoch 8/10\n",
            "20508/20508 [==============================] - 106s 5ms/step - loss: 0.0493 - acc: 0.9838\n",
            "Epoch 9/10\n",
            "20508/20508 [==============================] - 107s 5ms/step - loss: 0.0316 - acc: 0.9900\n",
            "Epoch 10/10\n",
            "20508/20508 [==============================] - 106s 5ms/step - loss: 0.0277 - acc: 0.9915\n",
            "2278/2278 [==============================] - 3s 1ms/step\n",
            "Epoch 1/10\n",
            "20508/20508 [==============================] - 109s 5ms/step - loss: 0.3142 - acc: 0.8692\n",
            "Epoch 2/10\n",
            "20508/20508 [==============================] - 108s 5ms/step - loss: 0.2246 - acc: 0.9070\n",
            "Epoch 3/10\n",
            "20508/20508 [==============================] - 107s 5ms/step - loss: 0.1956 - acc: 0.9198\n",
            "Epoch 4/10\n",
            "20508/20508 [==============================] - 107s 5ms/step - loss: 0.1657 - acc: 0.9324\n",
            "Epoch 5/10\n",
            "20508/20508 [==============================] - 108s 5ms/step - loss: 0.1382 - acc: 0.9445\n",
            "Epoch 6/10\n",
            "20508/20508 [==============================] - 107s 5ms/step - loss: 0.1111 - acc: 0.9575\n",
            "Epoch 7/10\n",
            "20508/20508 [==============================] - 107s 5ms/step - loss: 0.0835 - acc: 0.9686\n",
            "Epoch 8/10\n",
            "20508/20508 [==============================] - 107s 5ms/step - loss: 0.0597 - acc: 0.9791\n",
            "Epoch 9/10\n",
            "20508/20508 [==============================] - 108s 5ms/step - loss: 0.0416 - acc: 0.9867\n",
            "Epoch 10/10\n",
            "20508/20508 [==============================] - 107s 5ms/step - loss: 0.0305 - acc: 0.9908\n",
            "2278/2278 [==============================] - 3s 1ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'estimator': (<keras.wrappers.scikit_learn.KerasClassifier at 0x7fd1ec43c748>,\n",
              "  <keras.wrappers.scikit_learn.KerasClassifier at 0x7fd2450a20b8>,\n",
              "  <keras.wrappers.scikit_learn.KerasClassifier at 0x7fd1e12a9438>,\n",
              "  <keras.wrappers.scikit_learn.KerasClassifier at 0x7fd1dda61e10>,\n",
              "  <keras.wrappers.scikit_learn.KerasClassifier at 0x7fd22784db38>,\n",
              "  <keras.wrappers.scikit_learn.KerasClassifier at 0x7fd1be6307f0>,\n",
              "  <keras.wrappers.scikit_learn.KerasClassifier at 0x7fd1be1cc860>,\n",
              "  <keras.wrappers.scikit_learn.KerasClassifier at 0x7fd1752052b0>,\n",
              "  <keras.wrappers.scikit_learn.KerasClassifier at 0x7fd174e7f0f0>,\n",
              "  <keras.wrappers.scikit_learn.KerasClassifier at 0x7fd174a70e48>),\n",
              " 'fit_time': array([1102.0127604 , 1088.00492692, 1083.28889608, 1088.39755249,\n",
              "        1109.52912617, 1128.83751106, 1103.56242037, 1102.50659418,\n",
              "        1071.64536595, 1078.35892987]),\n",
              " 'score_time': array([2.38141727, 2.47667694, 2.72192764, 2.62916875, 2.79354215,\n",
              "        2.65654922, 2.76153636, 2.94455934, 2.87201262, 2.92859626]),\n",
              " 'test_f1': array([0.        , 0.        , 0.        , 0.        , 0.01277955,\n",
              "        0.94368482, 0.92572506, 0.9458584 , 0.95528548, 0.95910441]),\n",
              " 'test_precision': array([0.        , 0.        , 0.        , 0.        , 0.00643087,\n",
              "        1.        , 1.        , 1.        , 1.        , 1.        ]),\n",
              " 'test_recall': array([0.        , 0.        , 0.        , 0.        , 1.        ,\n",
              "        0.89337429, 0.86172081, 0.89727831, 0.9143986 , 0.9214223 ])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTYi_dzd3VJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models = scores[\"estimator\"]\n",
        "def save_model(model, split_num, model_name_type):\n",
        "    # saving model\n",
        "    model_filename = f'{model_name_type}_split_{split_num}.h5'\n",
        "    # saving weights\n",
        "    model.save(f\"/content/gdrive/My Drive/Colab Notebooks/{model_filename}\", overwrite=True)\n",
        "\n",
        "\n",
        "for split_num, model in enumerate(models):\n",
        "  # print(model.model)\n",
        "  save_model(model.model, split_num+1, \"simple_lstm\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzdgIkogzluY",
        "colab_type": "code",
        "outputId": "5f9031c9-0265-4525-e67a-eea0647c879b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(\"Simple LSTM:\")\n",
        "predictions = simple_lstm.predict(X_test, verbose=1)\n",
        "print(model_metrics(predictions,y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Simple LSTM:\n",
            "5697/5697 [==============================] - 8s 1ms/step\n",
            "{'f1_score': 0.9064493267186392, 'precision_score': 0.9116179615110478, 'recall_score': 0.9013389711064129, 'roc_auc_score': 0.9072976772286174}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DYrTHw_3Uf-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOJzn4BjIT0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" CNN Model \"\"\"\n",
        "from keras import regularizers\n",
        "\n",
        "def create_cnn_model(num_filters, kernal_sizes, neurons, is_trainable):\n",
        "\n",
        "  #model parameters\n",
        "  max_length = 150\n",
        "  weight_decay = 1e-4\n",
        "\n",
        "  #CNN architecture\n",
        "  print(\"training CNN ...\")\n",
        "  cnn_model = Sequential()\n",
        "  cnn_model.add(Embedding(vocab_size, embedding_size,\n",
        "            weights=[embedding_matrix], input_length=max_length, trainable=is_trainable))\n",
        "  cnn_model.add(Conv1D(num_filters, kernal_sizes, activation='relu', padding='same'))\n",
        "  cnn_model.add(MaxPooling1D(2))\n",
        "  cnn_model.add(Conv1D(num_filters, kernal_sizes, activation='relu', padding='same'))\n",
        "  cnn_model.add(GlobalMaxPooling1D())\n",
        "  cnn_model.add(Dense(neurons, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  cnn_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  cnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return cnn_model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3lZEgUFTAA3",
        "colab_type": "code",
        "outputId": "35bab9d0-c3f1-4690-8a9c-3ecc6e74be1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "source": [
        "# Parameter Tuning for CNN\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "epochs = [150,300]\n",
        "batch_size = [16]\n",
        "num_filters = [32, 64, 128]\n",
        "kernal_sizes = [2,3,4]\n",
        "is_trainable = [False, True]\n",
        "neurons = [16, 32, 64]\n",
        "\n",
        "\n",
        "# Test SGD optimizer ---> https://arxiv.org/pdf/1705.08292.pdf\n",
        "\n",
        "param_grid = dict(epochs=epochs, batch_size=batch_size, kernal_sizes= kernal_sizes, num_filters=num_filters, is_trainable=is_trainable,neurons=neurons)\n",
        "\n",
        "cnn_model = KerasClassifier(build_fn=create_cnn_model, verbose=1)\n",
        "grid = GridSearchCV(estimator=cnn_model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(X_train, y_train)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e709924b69b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernal_sizes\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mkernal_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_filters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_filters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_trainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_trainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mneurons\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneurons\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mcnn_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKerasClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_cnn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcnn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mgrid_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'create_cnn_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLz6tlKYITr2",
        "colab_type": "code",
        "outputId": "d81ca6e4-946b-4d62-d672-751e81f74361",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "\"\"\" Train CNN w/o parameter tuning\"\"\"\n",
        "\n",
        "  #training params\n",
        "  batch_size = 256 \n",
        "  num_epochs = 8 \n",
        "  #model parameters\n",
        "  num_filters = 64 \n",
        "  max_length = 150\n",
        "  weight_decay = 1e-4\n",
        "\n",
        "  #CNN architecture\n",
        "  print(\"training CNN ...\")\n",
        "  cnn_model = Sequential()\n",
        "  cnn_model.add(Embedding(vocab_size, embedding_size,\n",
        "            weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
        "  cnn_model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
        "  cnn_model.add(MaxPooling1D(2))\n",
        "  cnn_model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
        "  cnn_model.add(GlobalMaxPooling1D())\n",
        "  cnn_model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  cnn_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  cnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "#define callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
        "callbacks_list = [early_stopping]\n",
        "\n",
        "#model training\n",
        "hist = cnn_model.fit(X_train, y_train, batch_size=batch_size, epochs=200, callbacks=callbacks_list, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:842: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
            "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pmgb1ZJiITp5",
        "colab_type": "code",
        "outputId": "c687dd7d-f1bc-491f-d9d5-75b8d2bad3b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(\"CNN\")\n",
        "predictions = cnn_model.predict(X_test, verbose=1)\n",
        "print(model_metrics(predictions,y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN\n",
            "5697/5697 [==============================] - 1s 196us/step\n",
            "{'f1_score': 0.8960787761561455, 'precision_score': 0.8943488943488943, 'recall_score': 0.897815362931642, 'roc_auc_score': 0.89626689797509}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygpCAWgiOyJj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEbddeIaOyP5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJF6Zx-6OyMr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-u2rrfVobZZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lS68T_BxobTg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(grid_result.best_params_)\n",
        "print(grid_result.best_score_)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}