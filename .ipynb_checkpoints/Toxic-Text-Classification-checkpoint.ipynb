{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import urllib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes and Questions:\n",
    "* Every comment has 10 unique annotators that label a comment toxic/not toxic and give it a score of toxicity\n",
    "    * Should I feed the model the same comment with different classifications? Or should I only have the dataset contain one of each comment with the associtated label. The label would be the computed average of each unique annotator's toxicity score with >= 0.5 classified as \"Toxic\" and < 0.5 being \"Non-Toxic\"\n",
    "* Strong class imbalance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_csv('data/toxicity_annotated_comments.tsv', sep='\\t', index_col = 0)\n",
    "annotations = pd.read_csv('data/toxicity_annotations.tsv',  sep='\\t')\n",
    "demographics = pd.read_csv('data/toxicity_worker_demographics.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>year</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>ns</th>\n",
       "      <th>sample</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rev_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2232.0</th>\n",
       "      <td>This:NEWLINE_TOKEN:One can make an analogy in ...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4216.0</th>\n",
       "      <td>`NEWLINE_TOKENNEWLINE_TOKEN:Clarification for ...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8953.0</th>\n",
       "      <td>Elected or Electoral? JHK</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26547.0</th>\n",
       "      <td>`This is such a fun entry.   DevotchkaNEWLINE_...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28959.0</th>\n",
       "      <td>Please relate the ozone hole to increases in c...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   comment  year  logged_in  \\\n",
       "rev_id                                                                        \n",
       "2232.0   This:NEWLINE_TOKEN:One can make an analogy in ...  2002       True   \n",
       "4216.0   `NEWLINE_TOKENNEWLINE_TOKEN:Clarification for ...  2002       True   \n",
       "8953.0                           Elected or Electoral? JHK  2002      False   \n",
       "26547.0  `This is such a fun entry.   DevotchkaNEWLINE_...  2002       True   \n",
       "28959.0  Please relate the ozone hole to increases in c...  2002       True   \n",
       "\n",
       "              ns  sample  split  \n",
       "rev_id                           \n",
       "2232.0   article  random  train  \n",
       "4216.0      user  random  train  \n",
       "8953.0   article  random   test  \n",
       "26547.0  article  random  train  \n",
       "28959.0  article  random   test  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove newline and tab tokens\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>year</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>ns</th>\n",
       "      <th>sample</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rev_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2232.0</th>\n",
       "      <td>This: :One can make an analogy in mathematical...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4216.0</th>\n",
       "      <td>`  :Clarification for you  (and Zundark's righ...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8953.0</th>\n",
       "      <td>Elected or Electoral? JHK</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26547.0</th>\n",
       "      <td>`This is such a fun entry.   Devotchka  I once...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28959.0</th>\n",
       "      <td>Please relate the ozone hole to increases in c...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   comment  year  logged_in  \\\n",
       "rev_id                                                                        \n",
       "2232.0   This: :One can make an analogy in mathematical...  2002       True   \n",
       "4216.0   `  :Clarification for you  (and Zundark's righ...  2002       True   \n",
       "8953.0                           Elected or Electoral? JHK  2002      False   \n",
       "26547.0  `This is such a fun entry.   Devotchka  I once...  2002       True   \n",
       "28959.0  Please relate the ozone hole to increases in c...  2002       True   \n",
       "\n",
       "              ns  sample  split  \n",
       "rev_id                           \n",
       "2232.0   article  random  train  \n",
       "4216.0      user  random  train  \n",
       "8953.0   article  random   test  \n",
       "26547.0  article  random  train  \n",
       "28959.0  article  random   test  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels a comment as an atack if the majority of annoatators did so\n",
    "labels = annotations.groupby('rev_id')['toxicity'].mean() > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments = comments.groupby('rev_id')\n",
    "# join labels and comments\n",
    "comments['is_toxic'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>year</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>ns</th>\n",
       "      <th>sample</th>\n",
       "      <th>split</th>\n",
       "      <th>is_toxic</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rev_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2232.0</th>\n",
       "      <td>This: :One can make an analogy in mathematical...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4216.0</th>\n",
       "      <td>`  :Clarification for you  (and Zundark's righ...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8953.0</th>\n",
       "      <td>Elected or Electoral? JHK</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26547.0</th>\n",
       "      <td>`This is such a fun entry.   Devotchka  I once...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28959.0</th>\n",
       "      <td>Please relate the ozone hole to increases in c...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>test</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   comment  year  logged_in  \\\n",
       "rev_id                                                                        \n",
       "2232.0   This: :One can make an analogy in mathematical...  2002       True   \n",
       "4216.0   `  :Clarification for you  (and Zundark's righ...  2002       True   \n",
       "8953.0                           Elected or Electoral? JHK  2002      False   \n",
       "26547.0  `This is such a fun entry.   Devotchka  I once...  2002       True   \n",
       "28959.0  Please relate the ozone hole to increases in c...  2002       True   \n",
       "\n",
       "              ns  sample  split  is_toxic  \n",
       "rev_id                                     \n",
       "2232.0   article  random  train     False  \n",
       "4216.0      user  random  train     False  \n",
       "8953.0   article  random   test     False  \n",
       "26547.0  article  random  train     False  \n",
       "28959.0  article  random   test     False  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations[\"toxicity\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_id</th>\n",
       "      <th>worker_id</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>toxicity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>723</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>4000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>3989</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>3341</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>1574</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rev_id  worker_id  toxicity  toxicity_score\n",
       "0  2232.0        723         0             0.0\n",
       "1  2232.0       4000         0             0.0\n",
       "2  2232.0       3989         0             1.0\n",
       "3  2232.0       3341         0             0.0\n",
       "4  2232.0       1574         0             1.0"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1598289"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(annotations[\"rev_id\"].unique())\n",
    "len(annotations[\"rev_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1., -1.,  2., -2.])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations[\"toxicity_score\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x136056b70>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "\n",
    "\n",
    "len(comments[\"is_toxic\"])\n",
    "\n",
    "not_toxic_num = len(comments[comments[\"is_toxic\"] == False]) \n",
    "is_toxic_num = len(comments[comments[\"is_toxic\"] == True])\n",
    "\n",
    "\n",
    "sns.countplot(x=comments[\"is_toxic\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-145-44b5ee19dc9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# words = normalize(X_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mcomments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"comment\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"comment\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3589\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3590\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3591\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3593\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-145-44b5ee19dc9b>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# words = normalize(X_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mcomments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"comment\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"comment\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-145-44b5ee19dc9b>\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(words)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mnew_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Remove punctuation from list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mnew_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[^\\w\\s]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/re.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import re, string, unicodedata\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "def normalize(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "\n",
    "        # Remove non-ASCII characters\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        # Convert all characters to lowercase from list of tokenized words\n",
    "        new_word = new_word.lower()\n",
    "        # Remove punctuation from list        \n",
    "        new_word = re.sub(r'[^\\w\\s]', '', new_word)\n",
    "\n",
    "    return new_words\n",
    "\n",
    "# words = normalize(X_train)\n",
    "comments[\"comment\"] = comments[\"comment\"].apply(lambda x: \" \".join(normalize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = comments[\"comment\"]\n",
    "y = comments[\"is_toxic\"]\n",
    "\n",
    "# split the dataset into training and validation datasets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "# label encode the target  \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rev_id\n",
       "2232.0     This: :One can make an analogy in mathematical...\n",
       "4216.0     `  :Clarification for you  (and Zundark's righ...\n",
       "8953.0                             Elected or Electoral? JHK\n",
       "26547.0    `This is such a fun entry.   Devotchka  I once...\n",
       "28959.0    Please relate the ozone hole to increases in c...\n",
       "Name: comment, dtype: object"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count vectorizer \n",
    "# TODO: Try without removing stopwords when considering embeding vs frequency based approaches\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "count_vect.fit(X)\n",
    "X_train, X_test = count_vect.transform(X_train), count_vect.transform(X_test)\n",
    "\n",
    "# count_vect = Tokenizer()\n",
    "# count_vect.fit_on_texts(X)\n",
    "\n",
    "# X_train, X_test = count_vect.texts_to_sequences(X_train), count_vect.texts_to_sequences(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natepill/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9411920982219102\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "logreg = Pipeline([('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "               ])\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "# print(classification_report(y_test, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "106989/106989 [==============================] - 242s 2ms/step - loss: 0.3570 - acc: 0.9160\n",
      "Epoch 2/10\n",
      "106989/106989 [==============================] - 244s 2ms/step - loss: 0.2103 - acc: 0.9479\n",
      "Epoch 3/10\n",
      "106989/106989 [==============================] - 251s 2ms/step - loss: 0.1497 - acc: 0.9565\n",
      "Epoch 4/10\n",
      "  7000/106989 [>.............................] - ETA: 3:50 - loss: 0.1267 - acc: 0.9650"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-e33130f882b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_dim = X_train.shape[1]  # Number of features\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=input_dim, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, verbose=True, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading GLOVE word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file = 'data/glove.6B/glove.6B.300d.txt'\n",
    "\n",
    "emb_dict = {}\n",
    "glove = open(glove_file)\n",
    "for line in glove:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vector = np.asarray(values[1:], dtype='float32')\n",
    "    emb_dict[word] = vector\n",
    "glove.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing for embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-db97f90f22fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Text to Sequence and padding for equal length vecotrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain_seq_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mtest_seq_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras_preprocessing/text.py\u001b[0m in \u001b[0;36mtexts_to_sequences\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \"\"\"\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtexts_to_sequences_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras_preprocessing/text.py\u001b[0m in \u001b[0;36mtexts_to_sequences_generator\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    300\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext_elem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext_elem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m                         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras_preprocessing/text.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    300\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext_elem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext_elem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m                         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Word index\n",
    "word_index = count_vect.word_index\n",
    "\n",
    "\n",
    "# Text to Sequence and padding for equal length vecotrs\n",
    "train_seq_x = pad_sequences(count_vect.texts_to_sequences(X_train), maxlen=100)\n",
    "test_seq_x = pad_sequences(count_vect.texts_to_sequences(X_test), maxlen=100)\n",
    "\n",
    "\n",
    "# token-embedding mapping\n",
    "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants for Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "NB_WORDS = len(word_index)  # Parameter indicating the number of words we'll put in the dictionary\n",
    "VAL_SIZE = X_train.shape[0]  # Size of the validation set\n",
    "BATCH_SIZE = 512  # Size of the batches used in the mini-batch gradient descent\n",
    "MAX_LEN = 100  # Maximum number of words in a sequence\n",
    "GLOVE_DIM = 300  # Number of dimensions of the GloVe word embeddings\n",
    "\n",
    "# y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206907"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 100, 8)            1655256   \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 801       \n",
      "=================================================================\n",
      "Total params: 1,656,057\n",
      "Trainable params: 1,656,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected embedding_3_input to have shape (100,) but got array with shape (186988,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-a5e788945c7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0memb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected embedding_3_input to have shape (100,) but got array with shape (186988,)"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Embedding\n",
    "\n",
    "\n",
    "emb_model = Sequential()\n",
    "emb_model.add(Embedding(NB_WORDS, 8, input_length=MAX_LEN))\n",
    "emb_model.add(Flatten())\n",
    "emb_model.add(Dense(1, activation='softmax'))\n",
    "# compile the model\n",
    "emb_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(emb_model.summary())\n",
    "# fit the model\n",
    "emb_model.fit(X_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding model with Glove weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model = Sequential()\n",
    "glove_model.add(Embedding(NB_WORDS, GLOVE_DIM, input_length=MAX_LEN))\n",
    "glove_model.add(Flatten())\n",
    "glove_model.add(Dense(1, activation='softmax'))\n",
    "glove_model.summary()\n",
    "\n",
    "glove_model.layers[0].set_weights([emb_matrix])\n",
    "glove_model.layers[0].trainable = False\n",
    "\n",
    "glove_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:** Detection of toxic pieces of text is an important task in moderation/flagging thoughts\n",
    "provided in thoughtexchange platform. In this project we aim to build an in-house deep learning\n",
    "model which could be used to apply to relatively small pieces of text and determines the toxicity\n",
    "probability of that text.\n",
    "\n",
    "**Data set:** https://figshare.com/articles/Wikipedia_Talk_Labels_Toxicity/4563973\n",
    "\n",
    "**Data Details:** \n",
    "* Wiki: https://meta.wikimedia.org/wiki/Research:Detox/Data_Release\n",
    "* Research Paper on documentation on the data collection and modeling methodology: https://arxiv.org/abs/1610.08914\n",
    "**Kaggle:** https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\n",
    "\n",
    "**Resources:** Check the following links to get resources:\n",
    "https://github.com/conversationai/perspectiveapi/blob/master/api_reference.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Project Description**\n",
    "\n",
    " Preprocessing and word embedding.\n",
    "     Compare using word2vec and GLoVe.\n",
    "\n",
    " Build a supervised framework for a two-way classifier which classifies toxic versus\n",
    "healthy pieces of text.\n",
    "\n",
    "     The output of the network is a probability score between 0 and 1 representing the\n",
    "    toxicity of the text.\n",
    "\n",
    " Apply different model architectures using the following networks to achieve the best\n",
    "outputs.\n",
    "\n",
    "     Convolutional neural network.\n",
    "     (Bidirectional) Long short term memory.\n",
    "     If time allows, Investigate state of the art approaches to obtain better results.\n",
    " Use keras Platform with tensorflow backend.\n",
    "     Programming language: python 3.\n",
    "    \n",
    " Tune the deep learning networks to improve the results.\n",
    "\n",
    " Compare outputs of different model architectures.\n",
    "     Provide precision, recall and/or sensitivity and specificity.\n",
    "     Provide area under the curve and/or F-score.\n",
    "    \n",
    " Perform n-fold cross-validation strategy to assess the outputs.\n",
    "\n",
    " Apply the built models on a set of unseen labeled thoughts from thoughtexchange to\n",
    "have a final evaluation of the selected method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
