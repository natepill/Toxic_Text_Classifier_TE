{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Dense, Input, LSTM, Bidirectional, Conv1D\n",
    "from keras.layers import Dropout, Embedding\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D, concatenate, SpatialDropout1D\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Dense, Input, LSTM, Bidirectional, Conv1D\n",
    "from keras.layers import Dropout, Embedding\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D, concatenate, SpatialDropout1D\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = 'data/glove.6B.300d.txt'\n",
    "df = pd.read_csv('data/train.csv').fillna(\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61457     \"\\n\\n STOP VANDALIZING \\n\\nPlease stop your di...\n",
       "28251     \"\\nSaid from someone who had some very offensi...\n",
       "42900     Media:Example.oggInsert non-formatted text her...\n",
       "75000     \"\\n\\n Good to see that a lot of this content i...\n",
       "126592                     Hight towers company for towers.\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution across 2 classes:\n",
      "108285 108285\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Replace NA values\n",
    "df['comment_text'].fillna(' ')\n",
    "\n",
    "# Separate input features and target\n",
    "y = df.loc[:, \"toxic\"]\n",
    "X = df.loc[:,'comment_text']\n",
    "\n",
    "\n",
    "# split into train and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# concatenate our training data back together\n",
    "X = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# separate minority and majority classes\n",
    "non_toxic = X[X[\"toxic\"]==0]\n",
    "toxic = X[X[\"toxic\"]==1]\n",
    "\n",
    "# upsample minority\n",
    "toxic_upsampled = resample(toxic,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(non_toxic), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "\n",
    "# combine majority and upsampled minority\n",
    "X_upsampled = pd.concat([non_toxic, toxic_upsampled])\n",
    "\n",
    "\n",
    "# AGAIN: Separate input features and target\n",
    "y = X_upsampled.loc[:, \"toxic\"]\n",
    "X = X_upsampled.loc[:,'comment_text']\n",
    "\n",
    "\n",
    "'''\n",
    "    QUESTION: Followed: https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18\n",
    "    for upsampling where it states to split before upsampling to avoid overfitting the data. After they split the data, they\n",
    "    resample the minority class, and then concatenate the results to a single Dataframe. In their model examples,\n",
    "    it seems as though they are training their models on the entirity of the dataset and resuing data that their model has\n",
    "    already seen for the test results, which should result in overfitting, right?\n",
    "    \n",
    "    \n",
    "    So once I have the balanced upsampled dataset, I've resplit the dataset in training and testing\n",
    "\n",
    "'''\n",
    "\n",
    "# split into train and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_non_toxic = X_upsampled.loc[X_upsampled[\"toxic\"] == 0]\n",
    "num_toxic = X_upsampled.loc[X_upsampled[\"toxic\"] == 1]\n",
    "\n",
    "\n",
    "print(\"Class Distribution across 2 classes:\")\n",
    "print(len(num_non_toxic),len(num_toxic))\n",
    "\n",
    "\n",
    "max_features=100000\n",
    "maxlen=150\n",
    "embed_size=300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize text + Prepare GloVe Embedding\n",
    "tokenizer = text.Tokenizer(num_words=max_features, lower=True)\n",
    "tokenizer.fit_on_texts(list(X_train))\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open(EMBEDDING_FILE, encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(max_features, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    \n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  app.launch_new_instance()\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(return_sequences=True, input_shape=(1, 32), units=32)`\n",
      "  app.launch_new_instance()\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=32, units=1)`\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-945716f2be4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m#The final model which gives the weighted sum:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Multiply each element with corresponding weight a[i][j][k] * b[i][j]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTimeDistributedMerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Sum the weighted elements.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "# NOTE: NOT CURRENTLY WORKING\n",
    "# I thought I'd keep this is in to visualize how a Sequential approach might work\n",
    "\n",
    "# from keras.layers.core import Merge\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Multiply\n",
    "\n",
    "input_dim = 32\n",
    "hidden = 32\n",
    "step = 1\n",
    "\n",
    "#The LSTM  model -  output_shape = (batch, step, hidden)\n",
    "model1 = Sequential()\n",
    "model1.add(LSTM(input_dim=input_dim, output_dim=hidden, input_length=step, return_sequences=True))\n",
    "\n",
    "#The weight model  - actual output shape  = (batch, step)\n",
    "# after reshape : output_shape = (batch, step,  hidden)\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(input_dim=input_dim, output_dim=step))\n",
    "model2.add(Activation('softmax')) # Learn a probability distribution over each  step.\n",
    "#Reshape to match LSTM's output shape, so that we can do element-wise multiplication.\n",
    "model2.add(RepeatVector(hidden))\n",
    "model2.add(Permute((2, 1)))\n",
    "\n",
    "\n",
    "#The final model which gives the weighted sum:\n",
    "model = Sequential()\n",
    "model.add(Multiply([model1, model2]))  # Multiply each element with corresponding weight a[i][j][k] * b[i][j]\n",
    "model.add(TimeDistributedMerge('sum')) # Sum the weighted elements.\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:43: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Flatten, Activation, multiply, TimeDistributed, RepeatVector, Permute, Lambda\n",
    "from keras import backend as K\n",
    "\n",
    "units = 32\n",
    "max_length = 150\n",
    "vocab_size = embedding_matrix.shape[0]\n",
    "embedding_size = embedding_matrix.shape[1]\n",
    "\n",
    "\n",
    "_input = Input(shape=[max_length], dtype='int32')\n",
    "\n",
    "# get the embedding layer\n",
    "embedded = Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_size,\n",
    "        input_length=max_length,\n",
    "        trainable=False,\n",
    "        weights=[embedding_matrix]\n",
    "    )(_input)\n",
    "\n",
    "# Bidirectional(LSTM(128, return_sequences=True))\n",
    "# activations = LSTM(units, return_sequences=True)(embedded)\n",
    "# Bidirectional(LSTM(64, return_sequences=True))\n",
    "activations = Bidirectional(LSTM(32, return_sequences=True))(embedded)\n",
    "\n",
    "\n",
    "# compute importance for each step\n",
    "attention = TimeDistributed(Dense(1, activation='tanh'))(activations) \n",
    "attention = Flatten()(attention)\n",
    "attention = Activation('softmax')(attention)\n",
    "# adjusting vector shapes for multiplication\n",
    "attention = RepeatVector(64)(attention)\n",
    "attention = Permute([2, 1])(attention)\n",
    "\n",
    "# apply the attention\n",
    "\n",
    "sent_representation = multiply([activations, attention])\n",
    "\n",
    "sent_representation = Lambda(lambda xin: K.sum(xin, axis=1))(sent_representation) \n",
    "\n",
    "probabilities = Dense(1, activation='sigmoid')(sent_representation)\n",
    "\n",
    "model = Model(input=_input, output=probabilities)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "162427/162427 [==============================] - 960s 6ms/step - loss: 0.3941 - acc: 0.8168\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d8491c18>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction\n",
    "batch_size = 32\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=1, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - 294s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "\n",
    "accuracy_score(y_test, predictions)\n",
    "\n",
    "\n",
    "f1_score(y_test, predictions)\n",
    "\n",
    "    \n",
    "recall_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = dict(epochs=[10,20,30], batch_size=[10, 20, 30], optimizer=['SGD', 'Adam'])\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_result.best_params_)\n",
    "print(grid_result.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "\n",
    "Dec 18\n",
    " \n",
    "* Email Milad and CC Farhad about GPU usage\n",
    "* Try Downsampling if GPU usage is not possible\n",
    "* Compare sole BiLSTM w/ model with Attn\n",
    "* Compare Eval metrics between lstm and cnn\n",
    "* Check Farhad email for parameters: loss function, activation of the output Dense layer\n",
    "* Make sure outputing probabilities\n",
    "* Generating confusion matrix for training and validation data\n",
    "* Try different n-grams (filter-size) for the CNN model \n",
    "--> Obtaining specficity and sensitvity\n",
    "* Check out Perspective API CNN implementation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "-----------------------------------------\n",
    "\n",
    "* Upsample the minority class\n",
    "\n",
    "* Eval Metrics:\n",
    "Select One\n",
    "Sensitivity(Recall) and specificity, Percision, F1 Score\n",
    "Calculate both for training and test\n",
    "\n",
    "AUC \n",
    "\n",
    "Focus on model eval for LSTM and then compare to CNN performance\n",
    "\n",
    "Hyperparamter tuning for LSTM and CNN models\n",
    "\n",
    "NOTE: there should be balance between sensitivity, specificity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
